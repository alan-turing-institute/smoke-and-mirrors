{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b1fef-96ee-4f72-84ed-ca963d6364ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b51d0d-7efd-4de2-9740-b38826695c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook containing several experiments on LLM response generation given a Modbus request.\n",
    "\n",
    "# To run the notebook, create a `.env` file at the root of the repo with the following line:\n",
    "# OPENAI_API_KEY=\"<INSERT YOUR KEY HERE>\"\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf04bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for prompting.\n",
    "\n",
    "OPENAI_MAX_TOKENS = 4096\n",
    "OPEN_AI_MAX_RESPONSE_TOKENS = 500\n",
    "OPENAI_MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "# from https://github.com/Azure/openai-samples/blob/main/Basic_Samples/Chat/chatGPT_managing_conversation.ipynb\n",
    "def num_tokens_from_messages(messages, model):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += (\n",
    "            4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        )\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens\n",
    "\n",
    "def send_message(\n",
    "    messages: list,\n",
    "    model_name: str,\n",
    "    system_prompt: str,\n",
    "    max_response_tokens=500,\n",
    "    temperature: float = 0.5,\n",
    ") -> str:\n",
    "    response = openai.chat.completions.create(\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt}, *messages],\n",
    "        model=model_name,\n",
    "        max_tokens=max_response_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    choice = response.choices[0]\n",
    "    if choice.message.content:\n",
    "        return choice.message.content\n",
    "    if choice.message.refusal:\n",
    "        return choice.message.refusal\n",
    "\n",
    "    msg = \"No content or refusal message in completion.\"\n",
    "    raise RuntimeError(msg)\n",
    "\n",
    "def send_chat_request(\n",
    "    request: str,\n",
    "    messages: list,\n",
    "    system_prompt: str,\n",
    "    model_name: str = OPENAI_MODEL_NAME,\n",
    "    max_response_tokens: int = OPEN_AI_MAX_RESPONSE_TOKENS,\n",
    "    temperature: float = 0.5,\n",
    ") -> str:\n",
    "    # add to messages\n",
    "    messages.append({\"role\": \"user\", \"content\": request})\n",
    "    # drop messages where needed\n",
    "    prompt_max_tokens = OPENAI_MAX_TOKENS - max_response_tokens\n",
    "    token_count = num_tokens_from_messages(messages, model_name)\n",
    "\n",
    "    # remove first message while over the token limit\n",
    "    while token_count > prompt_max_tokens:\n",
    "        messages.pop(0)\n",
    "        token_count = num_tokens_from_messages(messages, model_name)\n",
    "\n",
    "    return send_message(\n",
    "        messages,\n",
    "        system_prompt=system_prompt,\n",
    "        model_name=model_name,\n",
    "        max_response_tokens=max_response_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "def spaced(hex: str) -> str:\n",
    "    return \" \".join(hex[i:i + 2] for i in range(0, len(hex), 2))\n",
    "\n",
    "def function_code(hex: str) -> str:\n",
    "    \"\"\"Extracts the 1-byte function code.\"\"\"\n",
    "    return hex[14:16]\n",
    "\n",
    "def device_address(hex: str) -> str:\n",
    "    \"\"\"Extracts the device address field (last byte of the header).\"\"\"\n",
    "    return hex[12:14]\n",
    "\n",
    "def payload(hex: str) -> str:\n",
    "    \"\"\"Extracts the request payload.\"\"\"\n",
    "    return hex[16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Simple prompt.\n",
    "messages = []\n",
    "system_prompt = \"You will act as a Modbus TCP server.\"\n",
    "request = \"41DC0000000601010001\" # Read single coil\n",
    "prompt = f\"Give a valid response for the following Modbus request:\\n{spaced(request)}\"\n",
    "\n",
    "response = send_chat_request(\n",
    "    request=prompt,\n",
    "    messages=messages,\n",
    "    system_prompt=system_prompt,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c36ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: Explicit prompt.\n",
    "system_prompt = \"You will act as a Modbus TCP server. When given a Modbus request you will respond with a valid Modbus response as a hexadecimal string.\"\n",
    "request = \"970200000006010100040002\" # Read coils\n",
    "prompt = f\"Give a valid response for the following Modbus request including the 7-byte header (MBAP) and protocol data unit (PDU):\\n{spaced(request)}\"\n",
    "\n",
    "response = send_chat_request(\n",
    "    request=prompt,\n",
    "    messages=messages,\n",
    "    system_prompt=system_prompt,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa0d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3: Multistep with focus on valid protocol syntax.\n",
    "messages = []\n",
    "system_prompt = \"You will act as a Modbus TCP server.\"\n",
    "request = \"EE7200000006010100090004\" # Read coils\n",
    "prime_prompt = f\"In the modbus protocol, if the device address is {device_address(request)} and the function code is {function_code(request)} and the complete request is {spaced(request)}, how many bytes should the response PDU contain?\"  # noqa: E501\n",
    "\n",
    "response = send_chat_request(\n",
    "    request=prime_prompt,\n",
    "    messages=messages,\n",
    "    system_prompt=system_prompt,\n",
    ")\n",
    "print(response)\n",
    "\n",
    "prompt = f\"Provide a complete modbus response for the following request, including the header containing the Transaction Identifier, the Protocol Identifier, the Message Length and the Device Address:\\n{spaced(request)}\"\n",
    "response = send_chat_request(\n",
    "    request=prompt,\n",
    "    messages=messages,\n",
    "    system_prompt=system_prompt,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bbe1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 4: Multishot prompt.\n",
    "messages = []\n",
    "samples = [\n",
    "\t(\"E1A700000006010100070007\", \"E1A70000000401010100\"),\n",
    "\t(\"3BD300000006010100090002\", \"3BD30000000401010100\"),\n",
    "\t(\"B59200000006010200170005\", \"B5920000000401020100\"),\n",
    "\t(\"793F000000060102001C0004\", \"793F0000000401020100\"),\n",
    "\t(\"08EC00000006010200080006\", \"08EC0000000401020100\"),\n",
    "\t(\"0FC6000000060103000B0005\", \"0FC60000000D01030A00000000000000000000\"),\n",
    "\t(\"081C000000060103000B0001\", \"081C000000050103020000\"),\n",
    "\t(\"B13200000006010300110007\", \"B1320000001101030E0000000000000000000000000000\"),\n",
    "\t(\"F7EF00000006010300130001\", \"F7EF000000050103020000\"),\n",
    "\t(\"ED5600000006010300180007\", \"ED560000001101030E0000000000000000000000000000\"),\n",
    "\t(\"4C0F00000006010400180007\", \"4C0F0000001101040E0000000000000000000000000000\"),\n",
    "\t(\"2EBD000000060104000B0007\", \"2EBD0000001101040E0000000000000000000000000000\"),\n",
    "\t(\"5DBE00000006010400190006\", \"5DBE0000000F01040C000000000000000000000000\"),\n",
    "\t(\"3A94000000060104000B0007\", \"3A940000001101040E0000000000000000000000000000\"),\n",
    "\t(\"077C00000006010400120002\", \"077C0000000701040400000000\"),\n",
    "\t(\"F9370000000601050018FF00\", \"F9370000000601050018FF00\"),\n",
    "\t(\"7FE90000000601050000FF00\", \"7FE90000000601050000FF00\"),\n",
    "\t(\"210E00000006010500140000\", \"210E00000006010500140000\"),\n",
    "\t(\"204B00000006010600050D0C\", \"204B00000006010600050D0C\"),\n",
    "\t(\"9AD70000000601060010F6E3\", \"9AD70000000601060010F6E3\"),\n",
    "\t(\"F64A00000008010F001500060122\", \"F64A00000006010F00150006\"),\n",
    "\t(\"B5E100000008010F000A00020103\", \"B5E100000006010F000A0002\"),\n",
    "\t(\"886C00000008010F000200060114\", \"886C00000006010F00020006\"),\n",
    "\t(\"7F9000000008010F001500010101\", \"7F9000000006010F00150001\"),\n",
    "\t(\"666A00000008010F001400070103\", \"666A00000006010F00140007\"),\n",
    "\t(\"E8220000000B0110000C000204AB08CB1A\", \"E822000000060110000C0002\"),\n",
    "\t(\"09E10000000B011000010002041E4CAABB\", \"09E100000006011000010002\"),\n",
    "\t(\"242400000009011000070001025B93\", \"242400000006011000070001\"),\n",
    "\t(\"02F50000000D0110001E000306674B33512E48\", \"02F5000000060110001E0003\"),\n",
    "\t(\"AAC10000000B0110001300020449F11A6E\", \"AAC100000006011000130002\"),\n",
    "\t(\"E1A700000006010100070007\", \"E1A70000000401010100\"),\n",
    "\t(\"3BD300000006010100090002\", \"3BD30000000401010100\"),\n",
    "\t(\"03D5000000060101001F0005\", \"03D50000000401010100\"),\n",
    "\t(\"793F000000060102001C0004\", \"793F0000000401020100\"),\n",
    "\t(\"08EC00000006010200080006\", \"08EC0000000401020100\"),\n",
    "\t(\"0FC6000000060103000B0005\", \"0FC60000000D01030A00000000000000000000\"),\n",
    "\t(\"081C000000060103000B0001\", \"081C000000050103020000\"),\n",
    "\t(\"B13200000006010300110007\", \"B1320000001101030E0000000000000000000000000000\"),\n",
    "\t(\"F7EF00000006010300130001\", \"F7EF000000050103020000\"),\n",
    "\t(\"ED5600000006010300180007\", \"ED560000001101030E0000000000000000000000000000\"),\n",
    "\t(\"4C0F00000006010400180007\", \"4C0F0000001101040E0000000000000000000000000000\"),\n",
    "\t(\"2EBD000000060104000B0007\", \"2EBD0000001101040E0000000000000000000000000000\"),\n",
    "\t(\"5DBE00000006010400190006\", \"5DBE0000000F01040C000000000000000000000000\"),\n",
    "\t(\"3A94000000060104000B0007\", \"3A940000001101040E0000000000000000000000000000\"),\n",
    "\t(\"077C00000006010400120002\", \"077C0000000701040400000000\"),\n",
    "\t(\"F9370000000601050018FF00\", \"F9370000000601050018FF00\"),\n",
    "\t(\"7FE90000000601050000FF00\", \"7FE90000000601050000FF00\"),\n",
    "\t(\"210E00000006010500140000\", \"210E00000006010500140000\"),\n",
    "\t(\"204B00000006010600050D0C\", \"204B00000006010600050D0C\"),\n",
    "\t(\"9AD70000000601060010F6E3\", \"9AD70000000601060010F6E3\"),\n",
    "\t(\"F64A00000008010F001500060122\", \"F64A00000006010F00150006\"),\n",
    "\t(\"B5E100000008010F000A00020103\", \"B5E100000006010F000A0002\"),\n",
    "\t(\"886C00000008010F000200060114\", \"886C00000006010F00020006\"),\n",
    "\t(\"7F9000000008010F001500010101\", \"7F9000000006010F00150001\"),\n",
    "\t(\"666A00000008010F001400070103\", \"666A00000006010F00140007\"),\n",
    "\t(\"E8220000000B0110000C000204AB08CB1A\", \"E822000000060110000C0002\"),\n",
    "\t(\"09E10000000B011000010002041E4CAABB\", \"09E100000006011000010002\"),\n",
    "\t(\"242400000009011000070001025B93\", \"242400000006011000070001\"),\n",
    "\t(\"02F50000000D0110001E000306674B33512E48\", \"02F5000000060110001E0003\"),\n",
    "\t(\"AAC10000000B0110001300020449F11A6E\", \"AAC100000006011000130002\"),\n",
    "]\n",
    "system_prompt = \"You will act as a server on an OT network. When you get a valid request you will respond with a valid response as a hexadecimal string. \\nConsider the following examples of valid requests and responses:\\n\"\n",
    "for sample in samples:\n",
    "\tsystem_prompt += f\"Request: {spaced(sample[0])}. Response: {spaced(sample[1])}\\n\"\n",
    "\n",
    "request = \"AE4B00000006010100040003\" # Read coils\n",
    "prompt = f\"Give a valid response for the following Modbus request:\\n{spaced(request)}\"\n",
    "\n",
    "response = send_chat_request(\n",
    "    request=prompt,\n",
    "    messages=messages,\n",
    "    system_prompt=system_prompt,\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smoke-and-mirrors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
